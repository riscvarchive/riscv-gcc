/* DO NOT EDIT, please edit generator instead.
   This file was generated by gen-vector-iterator with the command:
   $ ./gen-vector-iterator -c > riscv-vector-iterator.h  */
/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector and integer modes.  */
#define _RVV_INT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI) \
  MACRO (8, f4, 32, vnx4qi, QI) \
  MACRO (8, f2, 16, vnx8qi, QI) \
  MACRO (8, 1, 8, vnx16qi, QI) \
  MACRO (8, 2, 4, vnx32qi, QI) \
  MACRO (8, 4, 2, vnx64qi, QI) \
  MACRO (8, 8, 1, vnx128qi, QI) \
  MACRO (16, f4, 64, vnx2hi, HI) \
  MACRO (16, f2, 32, vnx4hi, HI) \
  MACRO (16, 1, 16, vnx8hi, HI) \
  MACRO (16, 2, 8, vnx16hi, HI) \
  MACRO (16, 4, 4, vnx32hi, HI) \
  MACRO (16, 8, 2, vnx64hi, HI) \
  MACRO (32, f2, 64, vnx2si, SI) \
  MACRO (32, 1, 32, vnx4si, SI) \
  MACRO (32, 2, 16, vnx8si, SI) \
  MACRO (32, 4, 8, vnx16si, SI) \
  MACRO (32, 8, 4, vnx32si, SI) \
  MACRO (64, 1, 64, vnx2di, DI) \
  MACRO (64, 2, 32, vnx4di, DI) \
  MACRO (64, 4, 16, vnx8di, DI) \
  MACRO (64, 8, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_INT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer modes, and info for
   corresponding widening vector type.  */
#define _RVV_WINT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hi, HI) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hi, HI) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2si, SI) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_WINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer modes, and info for
   corresponding widening vector type.  */
#define _RVV_WRED_INT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, 8, 1, vnx128qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, 8, 2, vnx64hi, HI, 32, 1, vnx4si, SI) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 8, 4, vnx32si, SI, 64, 1, vnx2di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_WRED_INT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer modes, and info for
   corresponding quad widening vector type.  */
#define _RVV_QINT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 32, f2, vnx2si, SI) \
  MACRO (8, f4, 32, vnx4qi, QI, 32, 1, vnx4si, SI) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, 2, vnx8si, SI) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI) \
  MACRO (16, f4, 64, vnx2hi, HI, 64, 1, vnx2di, DI) \
  MACRO (16, f2, 32, vnx4hi, HI, 64, 2, vnx4di, DI) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_QINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer modes, and info for
   corresponding eightfold widening vector type.  */
#define _RVV_EINT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 64, 1, vnx2di, DI) \
  MACRO (8, f4, 32, vnx4qi, QI, 64, 2, vnx4di, DI) \
  MACRO (8, f2, 16, vnx8qi, QI, 64, 4, vnx8di, DI) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_EINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer point modes, and info for
   corresponding integer and vector type.  */
#define _RVV_INT_INDEX_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f8, vnx2qi, QI) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hi, HI) \
  MACRO (8, f8, 64, vnx2qi, QI, 32, f2, vnx2si, SI) \
  MACRO (8, f8, 64, vnx2qi, QI, 64, 1, vnx2di, DI) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f4, vnx4qi, QI) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hi, HI) \
  MACRO (8, f4, 32, vnx4qi, QI, 32, 1, vnx4si, SI) \
  MACRO (8, f4, 32, vnx4qi, QI, 64, 2, vnx4di, DI) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f2, vnx8qi, QI) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, 2, vnx8si, SI) \
  MACRO (8, f2, 16, vnx8qi, QI, 64, 4, vnx8di, DI) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 2, vnx32qi, QI) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 4, vnx64qi, QI) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 8, vnx128qi, QI) \
  MACRO (16, f4, 64, vnx2hi, HI, 8, f8, vnx2qi, QI) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, f4, vnx2hi, HI) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2si, SI) \
  MACRO (16, f4, 64, vnx2hi, HI, 64, 1, vnx2di, DI) \
  MACRO (16, f2, 32, vnx4hi, HI, 8, f4, vnx4qi, QI) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, f2, vnx4hi, HI) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI) \
  MACRO (16, f2, 32, vnx4hi, HI, 64, 2, vnx4di, DI) \
  MACRO (16, 1, 16, vnx8hi, HI, 8, f2, vnx8qi, QI) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 2, 8, vnx16hi, HI, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI) \
  MACRO (16, 4, 4, vnx32hi, HI, 8, 2, vnx32qi, QI) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 4, vnx32hi, HI) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI) \
  MACRO (16, 8, 2, vnx64hi, HI, 8, 4, vnx64qi, QI) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 8, vnx64hi, HI) \
  MACRO (32, f2, 64, vnx2si, SI, 8, f8, vnx2qi, QI) \
  MACRO (32, f2, 64, vnx2si, SI, 16, f4, vnx2hi, HI) \
  MACRO (32, f2, 64, vnx2si, SI, 32, f2, vnx2si, SI) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI) \
  MACRO (32, 1, 32, vnx4si, SI, 8, f4, vnx4qi, QI) \
  MACRO (32, 1, 32, vnx4si, SI, 16, f2, vnx4hi, HI) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 16, vnx8si, SI, 8, f2, vnx8qi, QI) \
  MACRO (32, 2, 16, vnx8si, SI, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 8, vnx16si, SI, 8, 1, vnx16qi, QI) \
  MACRO (32, 4, 8, vnx16si, SI, 16, 2, vnx16hi, HI) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 4, vnx16si, SI) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI) \
  MACRO (32, 8, 4, vnx32si, SI, 8, 2, vnx32qi, QI) \
  MACRO (32, 8, 4, vnx32si, SI, 16, 4, vnx32hi, HI) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 8, vnx32si, SI) \
  MACRO (64, 1, 64, vnx2di, DI, 8, f8, vnx2qi, QI) \
  MACRO (64, 1, 64, vnx2di, DI, 16, f4, vnx2hi, HI) \
  MACRO (64, 1, 64, vnx2di, DI, 32, f2, vnx2si, SI) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 2, 32, vnx4di, DI, 8, f4, vnx4qi, QI) \
  MACRO (64, 2, 32, vnx4di, DI, 16, f2, vnx4hi, HI) \
  MACRO (64, 2, 32, vnx4di, DI, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 2, vnx4di, DI) \
  MACRO (64, 4, 16, vnx8di, DI, 8, f2, vnx8qi, QI) \
  MACRO (64, 4, 16, vnx8di, DI, 16, 1, vnx8hi, HI) \
  MACRO (64, 4, 16, vnx8di, DI, 32, 2, vnx8si, SI) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 4, vnx8di, DI) \
  MACRO (64, 8, 8, vnx16di, DI, 8, 1, vnx16qi, QI) \
  MACRO (64, 8, 8, vnx16di, DI, 16, 2, vnx16hi, HI) \
  MACRO (64, 8, 8, vnx16di, DI, 32, 4, vnx16si, SI) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 8, vnx128qi, QI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer point modes, and info for
   corresponding integer and vector type.  */
#define _RVV_INT_REINT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f4, vnx4qi) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f2, vnx8qi) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 1, vnx16qi) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 2, vnx32qi) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 4, vnx64qi) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 8, vnx128qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f4, vnx2hi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f8, vnx2qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f2, vnx8qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 1, vnx16qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 2, vnx32qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 4, vnx64qi) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 8, vnx128qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, f2, vnx4hi) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, f2, vnx2si) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f8, vnx2qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f4, vnx4qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 1, vnx16qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 2, vnx32qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 4, vnx64qi) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 8, vnx128qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 1, vnx4si) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 1, vnx2di) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f8, vnx2qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f4, vnx4qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f2, vnx8qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 2, vnx32qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 4, vnx64qi) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 8, vnx128qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 2, vnx16hi) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 2, vnx8si) \
  MACRO (8, 2, 4, vnx32qi, QI, 64, 2, vnx4di) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f8, vnx2qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f4, vnx4qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f2, vnx8qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 1, vnx16qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 4, vnx64qi) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 8, vnx128qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 4, vnx32hi) \
  MACRO (8, 4, 2, vnx64qi, QI, 32, 4, vnx16si) \
  MACRO (8, 4, 2, vnx64qi, QI, 64, 4, vnx8di) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f8, vnx2qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f4, vnx4qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f2, vnx8qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 1, vnx16qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 2, vnx32qi) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 8, vnx128qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 16, 8, vnx64hi) \
  MACRO (8, 8, 1, vnx128qi, QI, 32, 8, vnx32si) \
  MACRO (8, 8, 1, vnx128qi, QI, 64, 8, vnx16di) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f8, vnx2qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f4, vnx4qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f2, vnx8qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 1, vnx16qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 2, vnx32qi) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 4, vnx64qi) \
  MACRO (16, f4, 64, vnx2hi, HI, 8, f4, vnx4qi) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, f2, vnx4hi) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 1, vnx8hi) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 2, vnx16hi) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 4, vnx32hi) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 8, vnx64hi) \
  MACRO (16, f2, 32, vnx4hi, HI, 8, f2, vnx8qi) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, f2, vnx2si) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, f4, vnx2hi) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 1, vnx8hi) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 2, vnx16hi) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 4, vnx32hi) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 8, vnx64hi) \
  MACRO (16, 1, 16, vnx8hi, HI, 8, 1, vnx16qi) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 1, vnx2di) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, f4, vnx2hi) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, f2, vnx4hi) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 2, vnx16hi) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 4, vnx32hi) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 8, vnx64hi) \
  MACRO (16, 2, 8, vnx16hi, HI, 8, 2, vnx32qi) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 2, vnx8si) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 2, vnx4di) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, f4, vnx2hi) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, f2, vnx4hi) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 1, vnx8hi) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 4, vnx32hi) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 8, vnx64hi) \
  MACRO (16, 4, 4, vnx32hi, HI, 8, 4, vnx64qi) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 4, vnx16si) \
  MACRO (16, 4, 4, vnx32hi, HI, 64, 4, vnx8di) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, f4, vnx2hi) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, f2, vnx4hi) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 1, vnx8hi) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 2, vnx16hi) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 8, vnx64hi) \
  MACRO (16, 8, 2, vnx64hi, HI, 8, 8, vnx128qi) \
  MACRO (16, 8, 2, vnx64hi, HI, 32, 8, vnx32si) \
  MACRO (16, 8, 2, vnx64hi, HI, 64, 8, vnx16di) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, f4, vnx2hi) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, f2, vnx4hi) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 1, vnx8hi) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 2, vnx16hi) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 4, vnx32hi) \
  MACRO (32, f2, 64, vnx2si, SI, 8, f2, vnx8qi) \
  MACRO (32, f2, 64, vnx2si, SI, 16, f2, vnx4hi) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 1, vnx4si) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 2, vnx8si) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 4, vnx16si) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 8, vnx32si) \
  MACRO (32, 1, 32, vnx4si, SI, 8, 1, vnx16qi) \
  MACRO (32, 1, 32, vnx4si, SI, 16, 1, vnx8hi) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di) \
  MACRO (32, 1, 32, vnx4si, SI, 32, f2, vnx2si) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 2, vnx8si) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 4, vnx16si) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 8, vnx32si) \
  MACRO (32, 2, 16, vnx8si, SI, 8, 2, vnx32qi) \
  MACRO (32, 2, 16, vnx8si, SI, 16, 2, vnx16hi) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 2, vnx4di) \
  MACRO (32, 2, 16, vnx8si, SI, 32, f2, vnx2si) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 1, vnx4si) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 4, vnx16si) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 8, vnx32si) \
  MACRO (32, 4, 8, vnx16si, SI, 8, 4, vnx64qi) \
  MACRO (32, 4, 8, vnx16si, SI, 16, 4, vnx32hi) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 4, vnx8di) \
  MACRO (32, 4, 8, vnx16si, SI, 32, f2, vnx2si) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 1, vnx4si) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 2, vnx8si) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 8, vnx32si) \
  MACRO (32, 8, 4, vnx32si, SI, 8, 8, vnx128qi) \
  MACRO (32, 8, 4, vnx32si, SI, 16, 8, vnx64hi) \
  MACRO (32, 8, 4, vnx32si, SI, 64, 8, vnx16di) \
  MACRO (32, 8, 4, vnx32si, SI, 32, f2, vnx2si) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 1, vnx4si) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 2, vnx8si) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 4, vnx16si) \
  MACRO (64, 1, 64, vnx2di, DI, 8, 1, vnx16qi) \
  MACRO (64, 1, 64, vnx2di, DI, 16, 1, vnx8hi) \
  MACRO (64, 1, 64, vnx2di, DI, 32, 1, vnx4si) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 2, vnx4di) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 4, vnx8di) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 8, vnx16di) \
  MACRO (64, 2, 32, vnx4di, DI, 8, 2, vnx32qi) \
  MACRO (64, 2, 32, vnx4di, DI, 16, 2, vnx16hi) \
  MACRO (64, 2, 32, vnx4di, DI, 32, 2, vnx8si) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 1, vnx2di) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 4, vnx8di) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 8, vnx16di) \
  MACRO (64, 4, 16, vnx8di, DI, 8, 4, vnx64qi) \
  MACRO (64, 4, 16, vnx8di, DI, 16, 4, vnx32hi) \
  MACRO (64, 4, 16, vnx8di, DI, 32, 4, vnx16si) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 1, vnx2di) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 2, vnx4di) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 8, vnx16di) \
  MACRO (64, 8, 8, vnx16di, DI, 8, 8, vnx128qi) \
  MACRO (64, 8, 8, vnx16di, DI, 16, 8, vnx64hi) \
  MACRO (64, 8, 8, vnx16di, DI, 32, 8, vnx32si) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 1, vnx2di) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 2, vnx4di) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 4, vnx8di) \

/* Same as above but with an extra argument.  */
#define _RVV_INT_REINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (8, f8, 64, vnx2qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 64, 4, vnx8di, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f8, vnx2qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (8, 8, 1, vnx128qi, QI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 8, f4, vnx4qi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 64, 4, vnx8di, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, f4, vnx2hi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hi, HI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 8, f2, vnx8qi, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 16, f2, vnx4hi, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 4, vnx8di, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 32, f2, vnx2si, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32si, SI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 8, 1, vnx16qi, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 16, 1, vnx8hi, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 32, 1, vnx4si, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 4, vnx8di, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2di, DI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 8, 2, vnx32qi, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 16, 2, vnx16hi, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 32, 2, vnx8si, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 4, vnx8di, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4di, DI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 8, 4, vnx64qi, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 16, 4, vnx32hi, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 32, 4, vnx16si, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8di, DI, 64, 8, vnx16di, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 8, 8, vnx128qi, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 16, 8, vnx64hi, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 32, 8, vnx32si, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 1, vnx2di, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 2, vnx4di, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16di, DI, 64, 4, vnx8di, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector and floating point modes.  */
#define _RVV_FLOAT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF) \
  MACRO (16, f2, 32, vnx4hf, HF) \
  MACRO (16, 1, 16, vnx8hf, HF) \
  MACRO (16, 2, 8, vnx16hf, HF) \
  MACRO (16, 4, 4, vnx32hf, HF) \
  MACRO (16, 8, 2, vnx64hf, HF) \
  MACRO (32, f2, 64, vnx2sf, SF) \
  MACRO (32, 1, 32, vnx4sf, SF) \
  MACRO (32, 2, 16, vnx8sf, SF) \
  MACRO (32, 4, 8, vnx16sf, SF) \
  MACRO (32, 8, 4, vnx32sf, SF) \
  MACRO (64, 1, 64, vnx2df, DF) \
  MACRO (64, 2, 32, vnx4df, DF) \
  MACRO (64, 4, 16, vnx8df, DF) \
  MACRO (64, 8, 8, vnx16df, DF) \

/* Same as above but with an extra argument.  */
#define _RVV_FLOAT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating modes, and info for
   corresponding widening vector type.  */
#define _RVV_WFLOAT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2sf, SF) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8sf, SF) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16sf, SF) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32sf, SF) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2df, DF) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4df, DF) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8df, DF) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16df, DF) \

/* Same as above but with an extra argument.  */
#define _RVV_WFLOAT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2sf, SF, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8sf, SF, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16sf, SF, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32sf, SF, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4df, DF, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8df, DF, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16df, DF, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating modes, and info for
   corresponding widening vector type.  */
#define _RVV_WRED_FLOAT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (16, 8, 2, vnx64hf, HF, 32, 1, vnx4sf, SF) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2df, DF) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 1, vnx2df, DF) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 1, vnx2df, DF) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 1, vnx2df, DF) \
  MACRO (32, 8, 4, vnx32sf, SF, 64, 1, vnx2df, DF) \

/* Same as above but with an extra argument.  */
#define _RVV_WRED_FLOAT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 64, 1, vnx2df, DF, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding integer and vector type.  */
#define _RVV_FLOAT_INT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, vnx2hi, HI) \
  MACRO (16, f2, 32, vnx4hf, HF, vnx4hi, HI) \
  MACRO (16, 1, 16, vnx8hf, HF, vnx8hi, HI) \
  MACRO (16, 2, 8, vnx16hf, HF, vnx16hi, HI) \
  MACRO (16, 4, 4, vnx32hf, HF, vnx32hi, HI) \
  MACRO (16, 8, 2, vnx64hf, HF, vnx64hi, HI) \
  MACRO (32, f2, 64, vnx2sf, SF, vnx2si, SI) \
  MACRO (32, 1, 32, vnx4sf, SF, vnx4si, SI) \
  MACRO (32, 2, 16, vnx8sf, SF, vnx8si, SI) \
  MACRO (32, 4, 8, vnx16sf, SF, vnx16si, SI) \
  MACRO (32, 8, 4, vnx32sf, SF, vnx32si, SI) \
  MACRO (64, 1, 64, vnx2df, DF, vnx2di, DI) \
  MACRO (64, 2, 32, vnx4df, DF, vnx4di, DI) \
  MACRO (64, 4, 16, vnx8df, DF, vnx8di, DI) \
  MACRO (64, 8, 8, vnx16df, DF, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_FLOAT_INT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, vnx2hi, HI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, vnx4hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, vnx64hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, vnx2si, SI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, vnx32si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, vnx8di, DI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding integer and vector type.  */
#define _RVV_FLOAT_WINT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2si, SI) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4si, SI) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2di, DI) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_FLOAT_WINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector type.  */
#define _RVV_WFLOAT_INT_ITERATOR(MACRO) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hf, HF) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hf, HF) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hf, HF) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hf, HF) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hf, HF) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hf, HF) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2sf, SF) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4sf, SF) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8sf, SF) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16sf, SF) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32sf, SF) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2df, DF) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4df, DF) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8df, DF) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16df, DF) \

/* Same as above but with an extra argument.  */
#define _RVV_WFLOAT_INT_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, f8, 64, vnx2qi, QI, 16, f4, vnx2hf, HF, __VA_ARGS__) \
  MACRO (8, f4, 32, vnx4qi, QI, 16, f2, vnx4hf, HF, __VA_ARGS__) \
  MACRO (8, f2, 16, vnx8qi, QI, 16, 1, vnx8hf, HF, __VA_ARGS__) \
  MACRO (8, 1, 8, vnx16qi, QI, 16, 2, vnx16hf, HF, __VA_ARGS__) \
  MACRO (8, 2, 4, vnx32qi, QI, 16, 4, vnx32hf, HF, __VA_ARGS__) \
  MACRO (8, 4, 2, vnx64qi, QI, 16, 8, vnx64hf, HF, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hi, HI, 32, f2, vnx2sf, SF, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hi, HI, 32, 1, vnx4sf, SF, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hi, HI, 32, 2, vnx8sf, SF, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hi, HI, 32, 4, vnx16sf, SF, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hi, HI, 32, 8, vnx32sf, SF, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2si, SI, 64, 1, vnx2df, DF, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4si, SI, 64, 2, vnx4df, DF, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8si, SI, 64, 4, vnx8df, DF, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16si, SI, 64, 8, vnx16df, DF, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point point modes, and info for
   corresponding floating point and vector type.  */
#define _RVV_FLOAT_INDEX_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, 8, f8, vnx2qi, QI) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, f4, vnx2hi, HI) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2si, SI) \
  MACRO (16, f4, 64, vnx2hf, HF, 64, 1, vnx2di, DI) \
  MACRO (16, f2, 32, vnx4hf, HF, 8, f4, vnx4qi, QI) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, f2, vnx4hi, HI) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4si, SI) \
  MACRO (16, f2, 32, vnx4hf, HF, 64, 2, vnx4di, DI) \
  MACRO (16, 1, 16, vnx8hf, HF, 8, f2, vnx8qi, QI) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 16, vnx8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 2, 8, vnx16hf, HF, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 8, vnx16hf, HF, 64, 8, vnx16di, DI) \
  MACRO (16, 4, 4, vnx32hf, HF, 8, 2, vnx32qi, QI) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 4, vnx32hi, HI) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI) \
  MACRO (16, 8, 2, vnx64hf, HF, 8, 4, vnx64qi, QI) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 8, vnx64hi, HI) \
  MACRO (32, f2, 64, vnx2sf, SF, 8, f8, vnx2qi, QI) \
  MACRO (32, f2, 64, vnx2sf, SF, 16, f4, vnx2hi, HI) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, f2, vnx2si, SI) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2di, DI) \
  MACRO (32, 1, 32, vnx4sf, SF, 8, f4, vnx4qi, QI) \
  MACRO (32, 1, 32, vnx4sf, SF, 16, f2, vnx4hi, HI) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 16, vnx8sf, SF, 8, f2, vnx8qi, QI) \
  MACRO (32, 2, 16, vnx8sf, SF, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 8, vnx16sf, SF, 8, 1, vnx16qi, QI) \
  MACRO (32, 4, 8, vnx16sf, SF, 16, 2, vnx16hi, HI) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 4, vnx16si, SI) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI) \
  MACRO (32, 8, 4, vnx32sf, SF, 8, 2, vnx32qi, QI) \
  MACRO (32, 8, 4, vnx32sf, SF, 16, 4, vnx32hi, HI) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 8, vnx32si, SI) \
  MACRO (64, 1, 64, vnx2df, DF, 8, f8, vnx2qi, QI) \
  MACRO (64, 1, 64, vnx2df, DF, 16, f4, vnx2hi, HI) \
  MACRO (64, 1, 64, vnx2df, DF, 32, f2, vnx2si, SI) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 2, 32, vnx4df, DF, 8, f4, vnx4qi, QI) \
  MACRO (64, 2, 32, vnx4df, DF, 16, f2, vnx4hi, HI) \
  MACRO (64, 2, 32, vnx4df, DF, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 2, vnx4di, DI) \
  MACRO (64, 4, 16, vnx8df, DF, 8, f2, vnx8qi, QI) \
  MACRO (64, 4, 16, vnx8df, DF, 16, 1, vnx8hi, HI) \
  MACRO (64, 4, 16, vnx8df, DF, 32, 2, vnx8si, SI) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 4, vnx8di, DI) \
  MACRO (64, 8, 8, vnx16df, DF, 8, 1, vnx16qi, QI) \
  MACRO (64, 8, 8, vnx16df, DF, 16, 2, vnx16hi, HI) \
  MACRO (64, 8, 8, vnx16df, DF, 32, 4, vnx16si, SI) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 8, vnx16di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 8, 4, vnx64qi, QI, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 8, vnx64hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 8, f8, vnx2qi, QI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 16, f4, vnx2hi, HI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 32, f2, vnx2si, SI, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 8, f4, vnx4qi, QI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 16, f2, vnx4hi, HI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 8, f2, vnx8qi, QI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 8, vnx16di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer point modes, and info for
   corresponding integer and vector type.  */
#define _RVV_FLOAT_REINT_ITERATOR(MACRO) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, f2, vnx4hf) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 1, vnx8hf) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 2, vnx16hf) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 4, vnx32hf) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 8, vnx64hf) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, f4, vnx2hf) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 1, vnx8hf) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 2, vnx16hf) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 4, vnx32hf) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 8, vnx64hf) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, f4, vnx2hf) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, f2, vnx4hf) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 2, vnx16hf) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 4, vnx32hf) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 8, vnx64hf) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, f4, vnx2hf) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, f2, vnx4hf) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 1, vnx8hf) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 4, vnx32hf) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 8, vnx64hf) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, f4, vnx2hf) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, f2, vnx4hf) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 1, vnx8hf) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 2, vnx16hf) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 8, vnx64hf) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, f4, vnx2hf) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, f2, vnx4hf) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 1, vnx8hf) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 2, vnx16hf) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 4, vnx32hf) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 1, vnx4sf) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 2, vnx8sf) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 4, vnx16sf) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 8, vnx32sf) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, f2, vnx2sf) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 2, vnx8sf) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 4, vnx16sf) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 8, vnx32sf) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, f2, vnx2sf) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 1, vnx4sf) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 4, vnx16sf) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 8, vnx32sf) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, f2, vnx2sf) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 1, vnx4sf) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 2, vnx8sf) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 8, vnx32sf) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, f2, vnx2sf) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 1, vnx4sf) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 2, vnx8sf) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 4, vnx16sf) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 2, vnx4df) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 4, vnx8df) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 8, vnx16df) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 1, vnx2df) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 4, vnx8df) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 8, vnx16df) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 1, vnx2df) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 2, vnx4df) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 8, vnx16df) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 1, vnx2df) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 2, vnx4df) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 4, vnx8df) \

/* Same as above but with an extra argument.  */
#define _RVV_FLOAT_REINT_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, f2, vnx4hf, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
  MACRO (16, f4, 64, vnx2hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, f4, vnx2hf, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
  MACRO (16, f2, 32, vnx4hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, f4, vnx2hf, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, f2, vnx4hf, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
  MACRO (16, 1, 16, vnx8hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, f4, vnx2hf, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, f2, vnx4hf, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
  MACRO (16, 2, 8, vnx16hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, f4, vnx2hf, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, f2, vnx4hf, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
  MACRO (16, 4, 4, vnx32hf, HF, 16, 8, vnx64hf, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, f4, vnx2hf, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, f2, vnx4hf, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 1, vnx8hf, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 2, vnx16hf, __VA_ARGS__) \
  MACRO (16, 8, 2, vnx64hf, HF, 16, 4, vnx32hf, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
  MACRO (32, f2, 64, vnx2sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, f2, vnx2sf, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
  MACRO (32, 1, 32, vnx4sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, f2, vnx2sf, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
  MACRO (32, 2, 16, vnx8sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, f2, vnx2sf, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
  MACRO (32, 4, 8, vnx16sf, SF, 32, 8, vnx32sf, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, f2, vnx2sf, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 1, vnx4sf, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 2, vnx8sf, __VA_ARGS__) \
  MACRO (32, 8, 4, vnx32sf, SF, 32, 4, vnx16sf, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 2, vnx4df, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 4, vnx8df, __VA_ARGS__) \
  MACRO (64, 1, 64, vnx2df, DF, 64, 8, vnx16df, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 1, vnx2df, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 4, vnx8df, __VA_ARGS__) \
  MACRO (64, 2, 32, vnx4df, DF, 64, 8, vnx16df, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 1, vnx2df, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 2, vnx4df, __VA_ARGS__) \
  MACRO (64, 4, 16, vnx8df, DF, 64, 8, vnx16df, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 1, vnx2df, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 2, vnx4df, __VA_ARGS__) \
  MACRO (64, 8, 8, vnx16df, DF, 64, 4, vnx8df, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer types, and info for
   corresponding widening vector LMUL.  */
#define _RVV_WLMUL(MACRO) \
  MACRO (8, 2, 1, 8, Q, q,VNx32Q, vnx32q) \
  MACRO (8, 4, 2, 4, Q, q,VNx64Q, vnx64q) \
  MACRO (8, 8, 4, 2, Q, q,VNx128Q, vnx128q) \
  MACRO (16, 2, 1, 16, H, h,VNx16H, vnx16h) \
  MACRO (16, 4, 2, 8, H, h,VNx32H, vnx32h) \
  MACRO (16, 8, 4, 4, H, h,VNx64H, vnx64h) \
  MACRO (32, 2, 1, 32, S, s,VNx8S, vnx8s) \
  MACRO (32, 4, 2, 16, S, s,VNx16S, vnx16s) \
  MACRO (32, 8, 4, 8, S, s,VNx32S, vnx32s) \
  MACRO (64, 2, 1, 64, D, d,VNx4D, vnx4d) \
  MACRO (64, 4, 2, 32, D, d,VNx8D, vnx8d) \
  MACRO (64, 8, 4, 16, D, d,VNx16D, vnx16d) \
  MACRO (8, 4, 1, 8, Q, q,VNx64Q, vnx64q) \
  MACRO (8, 8, 2, 4, Q, q,VNx128Q, vnx128q) \
  MACRO (16, 4, 1, 16, H, h,VNx32H, vnx32h) \
  MACRO (16, 8, 2, 8, H, h,VNx64H, vnx64h) \
  MACRO (32, 4, 1, 32, S, s,VNx16S, vnx16s) \
  MACRO (32, 8, 2, 16, S, s,VNx32S, vnx32s) \
  MACRO (64, 4, 1, 64, D, d,VNx8D, vnx8d) \
  MACRO (64, 8, 2, 32, D, d,VNx16D, vnx16d) \
  MACRO (8, 8, 1, 8, Q, q,VNx128Q, vnx128q) \
  MACRO (16, 8, 1, 16, H, h,VNx64H, vnx64h) \
  MACRO (32, 8, 1, 32, S, s,VNx32S, vnx32s) \
  MACRO (64, 8, 1, 64, D, d,VNx16D, vnx16d) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, float types, and info for
   corresponding widening vector LMUL.  */
#define _RVV_WLMUL_NO_SEW8(MACRO) \
  MACRO (16, 2, 1, 16, H, h,VNx16H, vnx16h) \
  MACRO (16, 4, 2, 8, H, h,VNx32H, vnx32h) \
  MACRO (16, 8, 4, 4, H, h,VNx64H, vnx64h) \
  MACRO (32, 2, 1, 32, S, s,VNx8S, vnx8s) \
  MACRO (32, 4, 2, 16, S, s,VNx16S, vnx16s) \
  MACRO (32, 8, 4, 8, S, s,VNx32S, vnx32s) \
  MACRO (64, 2, 1, 64, D, d,VNx4D, vnx4d) \
  MACRO (64, 4, 2, 32, D, d,VNx8D, vnx8d) \
  MACRO (64, 8, 4, 16, D, d,VNx16D, vnx16d) \
  MACRO (16, 4, 1, 16, H, h,VNx32H, vnx32h) \
  MACRO (16, 8, 2, 8, H, h,VNx64H, vnx64h) \
  MACRO (32, 4, 1, 32, S, s,VNx16S, vnx16s) \
  MACRO (32, 8, 2, 16, S, s,VNx32S, vnx32s) \
  MACRO (64, 4, 1, 64, D, d,VNx8D, vnx8d) \
  MACRO (64, 8, 2, 32, D, d,VNx16D, vnx16d) \
  MACRO (16, 8, 1, 16, H, h,VNx64H, vnx64h) \
  MACRO (32, 8, 1, 32, S, s,VNx32S, vnx32s) \
  MACRO (64, 8, 1, 64, D, d,VNx16D, vnx16d) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG(MACRO) \
  MACRO (8, 1, 2, 8, Q, q,VNx2x16Q, vnx2x16q) \
  MACRO (8, 1, 3, 8, Q, q,VNx3x16Q, vnx3x16q) \
  MACRO (8, 1, 4, 8, Q, q,VNx4x16Q, vnx4x16q) \
  MACRO (8, 1, 5, 8, Q, q,VNx5x16Q, vnx5x16q) \
  MACRO (8, 1, 6, 8, Q, q,VNx6x16Q, vnx6x16q) \
  MACRO (8, 1, 7, 8, Q, q,VNx7x16Q, vnx7x16q) \
  MACRO (8, 1, 8, 8, Q, q,VNx8x16Q, vnx8x16q) \
  MACRO (8, 2, 2, 4, Q, q,VNx2x32Q, vnx2x32q) \
  MACRO (8, 2, 3, 4, Q, q,VNx3x32Q, vnx3x32q) \
  MACRO (8, 2, 4, 4, Q, q,VNx4x32Q, vnx4x32q) \
  MACRO (8, 4, 2, 2, Q, q,VNx2x64Q, vnx2x64q) \
  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h) \
  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h) \
  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h) \
  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h) \
  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h) \
  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h) \
  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h) \
  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h) \
  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h) \
  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h) \
  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h) \
  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s) \
  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s) \
  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s) \
  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s) \
  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s) \
  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s) \
  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s) \
  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s) \
  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s) \
  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s) \
  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s) \
  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d) \
  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d) \
  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d) \
  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d) \
  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d) \
  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d) \
  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d) \
  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d) \
  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d) \
  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d) \
  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_ARG(MACRO, ...) \
  MACRO (8, 1, 2, 8, Q, q,VNx2x16Q, vnx2x16q, __VA_ARGS__) \
  MACRO (8, 1, 3, 8, Q, q,VNx3x16Q, vnx3x16q, __VA_ARGS__) \
  MACRO (8, 1, 4, 8, Q, q,VNx4x16Q, vnx4x16q, __VA_ARGS__) \
  MACRO (8, 1, 5, 8, Q, q,VNx5x16Q, vnx5x16q, __VA_ARGS__) \
  MACRO (8, 1, 6, 8, Q, q,VNx6x16Q, vnx6x16q, __VA_ARGS__) \
  MACRO (8, 1, 7, 8, Q, q,VNx7x16Q, vnx7x16q, __VA_ARGS__) \
  MACRO (8, 1, 8, 8, Q, q,VNx8x16Q, vnx8x16q, __VA_ARGS__) \
  MACRO (8, 2, 2, 4, Q, q,VNx2x32Q, vnx2x32q, __VA_ARGS__) \
  MACRO (8, 2, 3, 4, Q, q,VNx3x32Q, vnx3x32q, __VA_ARGS__) \
  MACRO (8, 2, 4, 4, Q, q,VNx4x32Q, vnx4x32q, __VA_ARGS__) \
  MACRO (8, 4, 2, 2, Q, q,VNx2x64Q, vnx2x64q, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, integer tuple modes, and info for
   corresponding integer and vector type.  */
#define _RVV_SEG_INT_INDEX_ITERATOR(MACRO) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 8, 1, vnx16qi, QI) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 16, 2, vnx16hi, HI) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 32, 4, vnx16si, SI) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 64, 8, vnx16di, DI) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 8, 2, vnx32qi, QI) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 16, 4, vnx32hi, HI) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 32, 8, vnx32si, SI) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 8, 2, vnx32qi, QI) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 16, 4, vnx32hi, HI) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 32, 8, vnx32si, SI) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 8, 2, vnx32qi, QI) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 16, 4, vnx32hi, HI) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 32, 8, vnx32si, SI) \
  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 8, 4, vnx64qi, QI) \
  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 16, 8, vnx64hi, HI) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 64, 4, vnx8di, DI) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 64, 8, vnx16di, DI) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 64, 8, vnx16di, DI) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 64, 8, vnx16di, DI) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 8, 2, vnx32qi, QI) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 16, 4, vnx32hi, HI) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 32, 8, vnx32si, SI) \
  MACRO (32, 1, 2, 32, vnx2x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 2, 32, vnx2x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 3, 32, vnx3x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 3, 32, vnx3x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 4, 32, vnx4x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 4, 32, vnx4x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 5, 32, vnx5x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 5, 32, vnx5x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 6, 32, vnx6x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 6, 32, vnx6x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 7, 32, vnx7x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 7, 32, vnx7x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 8, 32, vnx8x4si, SI, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 8, 32, vnx8x4si, SI, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 64, 4, vnx8di, DI) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 64, 4, vnx8di, DI) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 8, 1, vnx16qi, QI) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 16, 2, vnx16hi, HI) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 32, 4, vnx16si, SI) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 64, 8, vnx16di, DI) \
  MACRO (64, 1, 2, 64, vnx2x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 3, 64, vnx3x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 4, 64, vnx4x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 5, 64, vnx5x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 6, 64, vnx6x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 7, 64, vnx7x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 8, 64, vnx8x2di, DI, 64, 1, vnx2di, DI) \
  MACRO (64, 2, 2, 32, vnx2x4di, DI, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 2, 32, vnx2x4di, DI, 64, 2, vnx4di, DI) \
  MACRO (64, 2, 3, 32, vnx3x4di, DI, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 3, 32, vnx3x4di, DI, 64, 2, vnx4di, DI) \
  MACRO (64, 2, 4, 32, vnx4x4di, DI, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 4, 32, vnx4x4di, DI, 64, 2, vnx4di, DI) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 16, 1, vnx8hi, HI) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 32, 2, vnx8si, SI) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 64, 4, vnx8di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 2, 8, vnx2x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 3, 8, vnx3x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 4, 8, vnx4x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 5, 8, vnx5x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 6, 8, vnx6x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 7, 8, vnx7x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (8, 1, 8, 8, vnx8x16qi, QI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 2, 4, vnx2x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 3, 4, vnx3x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (8, 2, 4, 4, vnx4x32qi, QI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 8, 4, vnx64qi, QI, __VA_ARGS__) \
  MACRO (8, 4, 2, 2, vnx2x64qi, QI, 16, 8, vnx64hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, vnx2x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hi, HI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hi, HI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hi, HI, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, vnx2x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, vnx2x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, vnx3x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, vnx3x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, vnx4x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, vnx4x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, vnx5x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, vnx5x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, vnx6x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, vnx6x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, vnx7x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, vnx7x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, vnx8x4si, SI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, vnx8x4si, SI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8si, SI, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16si, SI, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, vnx2x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, vnx3x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, vnx4x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, vnx5x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, vnx6x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, vnx7x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, vnx8x2di, DI, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, vnx2x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, vnx2x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, vnx3x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, vnx3x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, vnx4x4di, DI, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, vnx4x4di, DI, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8di, DI, 64, 4, vnx8di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF2(MACRO) \
  MACRO (8, 1, 2, 8, Q, q, VNx2x16Q, vnx2x16q) \
  MACRO (8, 2, 2, 4, Q, q, VNx2x32Q, vnx2x32q) \
  MACRO (8, 4, 2, 2, Q, q, VNx2x64Q, vnx2x64q) \
  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h) \
  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h) \
  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h) \
  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s) \
  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s) \
  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s) \
  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d) \
  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d) \
  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF2_ARG(MACRO, ...) \
  MACRO (8, 1, 2, 8, Q, q, VNx2x16Q, vnx2x16q, __VA_ARGS__) \
  MACRO (8, 2, 2, 4, Q, q, VNx2x32Q, vnx2x32q, __VA_ARGS__) \
  MACRO (8, 4, 2, 2, Q, q, VNx2x64Q, vnx2x64q, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF3(MACRO) \
  MACRO (8, 1, 3, 8, Q, q, VNx3x16Q, vnx3x16q) \
  MACRO (8, 2, 3, 4, Q, q, VNx3x32Q, vnx3x32q) \
  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h) \
  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h) \
  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s) \
  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s) \
  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d) \
  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF3_ARG(MACRO, ...) \
  MACRO (8, 1, 3, 8, Q, q, VNx3x16Q, vnx3x16q, __VA_ARGS__) \
  MACRO (8, 2, 3, 4, Q, q, VNx3x32Q, vnx3x32q, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF4(MACRO) \
  MACRO (8, 1, 4, 8, Q, q, VNx4x16Q, vnx4x16q) \
  MACRO (8, 2, 4, 4, Q, q, VNx4x32Q, vnx4x32q) \
  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h) \
  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h) \
  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s) \
  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s) \
  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d) \
  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF4_ARG(MACRO, ...) \
  MACRO (8, 1, 4, 8, Q, q, VNx4x16Q, vnx4x16q, __VA_ARGS__) \
  MACRO (8, 2, 4, 4, Q, q, VNx4x32Q, vnx4x32q, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF5(MACRO) \
  MACRO (8, 1, 5, 8, Q, q, VNx5x16Q, vnx5x16q) \
  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h) \
  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s) \
  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF5_ARG(MACRO, ...) \
  MACRO (8, 1, 5, 8, Q, q, VNx5x16Q, vnx5x16q, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF6(MACRO) \
  MACRO (8, 1, 6, 8, Q, q, VNx6x16Q, vnx6x16q) \
  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h) \
  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s) \
  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF6_ARG(MACRO, ...) \
  MACRO (8, 1, 6, 8, Q, q, VNx6x16Q, vnx6x16q, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF7(MACRO) \
  MACRO (8, 1, 7, 8, Q, q, VNx7x16Q, vnx7x16q) \
  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h) \
  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s) \
  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF7_ARG(MACRO, ...) \
  MACRO (8, 1, 7, 8, Q, q, VNx7x16Q, vnx7x16q, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF8(MACRO) \
  MACRO (8, 1, 8, 8, Q, q, VNx8x16Q, vnx8x16q) \
  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h) \
  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s) \
  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF8_ARG(MACRO, ...) \
  MACRO (8, 1, 8, 8, Q, q, VNx8x16Q, vnx8x16q, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NO_SEW8(MACRO) \
  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h) \
  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h) \
  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h) \
  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h) \
  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h) \
  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h) \
  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h) \
  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h) \
  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h) \
  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h) \
  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h) \
  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s) \
  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s) \
  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s) \
  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s) \
  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s) \
  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s) \
  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s) \
  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s) \
  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s) \
  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s) \
  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s) \
  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d) \
  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d) \
  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d) \
  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d) \
  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d) \
  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d) \
  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d) \
  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d) \
  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d) \
  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d) \
  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 2, 16, H, h,VNx2x8H, vnx2x8h, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, H, h,VNx3x8H, vnx3x8h, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, H, h,VNx4x8H, vnx4x8h, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, H, h,VNx5x8H, vnx5x8h, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, H, h,VNx6x8H, vnx6x8h, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, H, h,VNx7x8H, vnx7x8h, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, H, h,VNx8x8H, vnx8x8h, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, H, h,VNx2x16H, vnx2x16h, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, H, h,VNx3x16H, vnx3x16h, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, H, h,VNx4x16H, vnx4x16h, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, H, h,VNx2x32H, vnx2x32h, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, S, s,VNx2x4S, vnx2x4s, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, S, s,VNx3x4S, vnx3x4s, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, S, s,VNx4x4S, vnx4x4s, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, S, s,VNx5x4S, vnx5x4s, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, S, s,VNx6x4S, vnx6x4s, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, S, s,VNx7x4S, vnx7x4s, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, S, s,VNx8x4S, vnx8x4s, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, S, s,VNx2x8S, vnx2x8s, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, S, s,VNx3x8S, vnx3x8s, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, S, s,VNx4x8S, vnx4x8s, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, S, s,VNx2x16S, vnx2x16s, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, D, d,VNx2x2D, vnx2x2d, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, D, d,VNx3x2D, vnx3x2d, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, D, d,VNx4x2D, vnx4x2d, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, D, d,VNx5x2D, vnx5x2d, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, D, d,VNx6x2D, vnx6x2d, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, D, d,VNx7x2D, vnx7x2d, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, D, d,VNx8x2D, vnx8x2d, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, D, d,VNx2x4D, vnx2x4d, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, D, d,VNx3x4D, vnx3x4d, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, D, d,VNx4x4D, vnx4x4d, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, D, d,VNx2x8D, vnx2x8d, __VA_ARGS__) \

/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
   along with its corresponding vector, float-point tuple modes, and info for
   corresponding integer and vector type.  */
#define _RVV_SEG_FLOAT_INDEX_ITERATOR(MACRO) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 16, 1, vnx8hi, HI) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 32, 2, vnx8si, SI) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 64, 4, vnx8di, DI) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 64, 8, vnx16di, DI) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 64, 8, vnx16di, DI) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 8, 1, vnx16qi, QI) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 16, 2, vnx16hi, HI) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 32, 4, vnx16si, SI) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 64, 8, vnx16di, DI) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 8, 2, vnx32qi, QI) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 16, 4, vnx32hi, HI) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 32, 8, vnx32si, SI) \
  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 32, 1, vnx4si, SI) \
  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 64, 2, vnx4di, DI) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 64, 4, vnx8di, DI) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 64, 4, vnx8di, DI) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 16, 1, vnx8hi, HI) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 32, 2, vnx8si, SI) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 64, 4, vnx8di, DI) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 8, 1, vnx16qi, QI) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 16, 2, vnx16hi, HI) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 32, 4, vnx16si, SI) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 64, 8, vnx16di, DI) \
  MACRO (64, 1, 2, 64, vnx2x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 3, 64, vnx3x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 4, 64, vnx4x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 5, 64, vnx5x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 6, 64, vnx6x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 7, 64, vnx7x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 1, 8, 64, vnx8x2df, DF, 64, 1, vnx2di, DI) \
  MACRO (64, 2, 2, 32, vnx2x4df, DF, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 2, 32, vnx2x4df, DF, 64, 2, vnx4di, DI) \
  MACRO (64, 2, 3, 32, vnx3x4df, DF, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 3, 32, vnx3x4df, DF, 64, 2, vnx4di, DI) \
  MACRO (64, 2, 4, 32, vnx4x4df, DF, 32, 1, vnx4si, SI) \
  MACRO (64, 2, 4, 32, vnx4x4df, DF, 64, 2, vnx4di, DI) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 16, 1, vnx8hi, HI) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 32, 2, vnx8si, SI) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 64, 4, vnx8di, DI) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 2, 16, vnx2x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 3, 16, vnx3x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 4, 16, vnx4x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 5, 16, vnx5x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 6, 16, vnx6x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 7, 16, vnx7x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (16, 1, 8, 16, vnx8x8hf, HF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, vnx2x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, vnx3x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, vnx4x16hf, HF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 8, 2, vnx32qi, QI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 16, 4, vnx32hi, HI, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, vnx2x32hf, HF, 32, 8, vnx32si, SI, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, vnx2x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, vnx3x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, vnx4x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, vnx5x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, vnx6x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, vnx7x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, vnx8x4sf, SF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, vnx2x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, vnx3x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, vnx4x8sf, SF, 64, 4, vnx8di, DI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 8, 1, vnx16qi, QI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 16, 2, vnx16hi, HI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 32, 4, vnx16si, SI, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, vnx2x16sf, SF, 64, 8, vnx16di, DI, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, vnx2x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, vnx3x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, vnx4x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, vnx5x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, vnx6x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, vnx7x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, vnx8x2df, DF, 64, 1, vnx2di, DI, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, vnx2x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, vnx2x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, vnx3x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, vnx3x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, vnx4x4df, DF, 32, 1, vnx4si, SI, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, vnx4x4df, DF, 64, 2, vnx4di, DI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 16, 1, vnx8hi, HI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 32, 2, vnx8si, SI, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, vnx2x8df, DF, 64, 4, vnx8di, DI, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF2_NO_SEW8(MACRO) \
  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h) \
  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h) \
  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h) \
  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s) \
  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s) \
  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s) \
  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d) \
  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d) \
  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF2_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 2, 16, H, h, VNx2x8H, vnx2x8h, __VA_ARGS__) \
  MACRO (16, 2, 2, 8, H, h, VNx2x16H, vnx2x16h, __VA_ARGS__) \
  MACRO (16, 4, 2, 4, H, h, VNx2x32H, vnx2x32h, __VA_ARGS__) \
  MACRO (32, 1, 2, 32, S, s, VNx2x4S, vnx2x4s, __VA_ARGS__) \
  MACRO (32, 2, 2, 16, S, s, VNx2x8S, vnx2x8s, __VA_ARGS__) \
  MACRO (32, 4, 2, 8, S, s, VNx2x16S, vnx2x16s, __VA_ARGS__) \
  MACRO (64, 1, 2, 64, D, d, VNx2x2D, vnx2x2d, __VA_ARGS__) \
  MACRO (64, 2, 2, 32, D, d, VNx2x4D, vnx2x4d, __VA_ARGS__) \
  MACRO (64, 4, 2, 16, D, d, VNx2x8D, vnx2x8d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF3_NO_SEW8(MACRO) \
  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h) \
  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h) \
  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s) \
  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s) \
  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d) \
  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF3_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 3, 16, H, h, VNx3x8H, vnx3x8h, __VA_ARGS__) \
  MACRO (16, 2, 3, 8, H, h, VNx3x16H, vnx3x16h, __VA_ARGS__) \
  MACRO (32, 1, 3, 32, S, s, VNx3x4S, vnx3x4s, __VA_ARGS__) \
  MACRO (32, 2, 3, 16, S, s, VNx3x8S, vnx3x8s, __VA_ARGS__) \
  MACRO (64, 1, 3, 64, D, d, VNx3x2D, vnx3x2d, __VA_ARGS__) \
  MACRO (64, 2, 3, 32, D, d, VNx3x4D, vnx3x4d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF4_NO_SEW8(MACRO) \
  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h) \
  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h) \
  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s) \
  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s) \
  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d) \
  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF4_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 4, 16, H, h, VNx4x8H, vnx4x8h, __VA_ARGS__) \
  MACRO (16, 2, 4, 8, H, h, VNx4x16H, vnx4x16h, __VA_ARGS__) \
  MACRO (32, 1, 4, 32, S, s, VNx4x4S, vnx4x4s, __VA_ARGS__) \
  MACRO (32, 2, 4, 16, S, s, VNx4x8S, vnx4x8s, __VA_ARGS__) \
  MACRO (64, 1, 4, 64, D, d, VNx4x2D, vnx4x2d, __VA_ARGS__) \
  MACRO (64, 2, 4, 32, D, d, VNx4x4D, vnx4x4d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF5_NO_SEW8(MACRO) \
  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h) \
  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s) \
  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF5_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 5, 16, H, h, VNx5x8H, vnx5x8h, __VA_ARGS__) \
  MACRO (32, 1, 5, 32, S, s, VNx5x4S, vnx5x4s, __VA_ARGS__) \
  MACRO (64, 1, 5, 64, D, d, VNx5x2D, vnx5x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF6_NO_SEW8(MACRO) \
  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h) \
  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s) \
  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF6_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 6, 16, H, h, VNx6x8H, vnx6x8h, __VA_ARGS__) \
  MACRO (32, 1, 6, 32, S, s, VNx6x4S, vnx6x4s, __VA_ARGS__) \
  MACRO (64, 1, 6, 64, D, d, VNx6x2D, vnx6x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF7_NO_SEW8(MACRO) \
  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h) \
  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s) \
  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF7_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 7, 16, H, h, VNx7x8H, vnx7x8h, __VA_ARGS__) \
  MACRO (32, 1, 7, 32, S, s, VNx7x4S, vnx7x4s, __VA_ARGS__) \
  MACRO (64, 1, 7, 64, D, d, VNx7x2D, vnx7x2d, __VA_ARGS__) \

/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
   along with its corresponding vector, floating point modes, and info for
   corresponding floating point and vector tuple type.  */
#define _RVV_SEG_NF8_NO_SEW8(MACRO) \
  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h) \
  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s) \
  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d) \

/* Same as above but with an extra argument.  */
#define _RVV_SEG_NF8_NO_SEW8_ARG(MACRO, ...) \
  MACRO (16, 1, 8, 16, H, h, VNx8x8H, vnx8x8h, __VA_ARGS__) \
  MACRO (32, 1, 8, 32, S, s, VNx8x4S, vnx8x4s, __VA_ARGS__) \
  MACRO (64, 1, 8, 64, D, d, VNx8x2D, vnx8x2d, __VA_ARGS__) \

